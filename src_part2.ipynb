{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2\n",
    "\n",
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best lambda 1e-07 test error 0.2888888716697693\n",
      "best lambda 0.001 test error 0.42222219705581665\n",
      "best lambda 1e-07 test error 0.29545456171035767\n",
      "best lambda 1e-06 test error 0.34090906381607056\n",
      "best lambda 1e-08 test error 0.20454543828964233\n",
      "best lambda 0.1 test error 0.15909093618392944\n",
      "best lambda 1e-09 test error 0.25\n",
      "best lambda 1e-08 test error 0.20454543828964233\n",
      "best lambda 1e-06 test error 0.27272725105285645\n",
      "best lambda 1e-06 test error 0.22727274894714355\n",
      "mean test error 0.2665656507015228\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import KFold\n",
    "from data import *\n",
    "\n",
    "np.random.seed(2)\n",
    "torch.manual_seed(2)\n",
    "\n",
    "# Define the Logistic Regression model in PyTorch.\n",
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.linear(x))\n",
    "\n",
    "# Convert the numpy arrays to PyTorch tensors\n",
    "X_torch = torch.from_numpy(X.astype(np.float32))\n",
    "\n",
    "# Convert the target to binary labels\n",
    "y = (y > np.median(y)).astype(int)  # 1 if y is above the median, 0 otherwise\n",
    "y_torch = torch.from_numpy(y.astype(np.float32)).view(-1, 1)\n",
    "\n",
    "K1, K2 = 10, 10  # Number of folds in the outer and inner cross-validation loops respectively\n",
    "outer_cv = KFold(n_splits=K1, shuffle=True)\n",
    "inner_cv = KFold(n_splits=K2, shuffle=True)\n",
    "\n",
    "lambdas = np.power(10.0, np.arange(-10, 0, 1)) # Range can not be higher than 0\n",
    "test_errors = []\n",
    "\n",
    "# Outer cross-validation loop\n",
    "for train_val_idx, test_idx in outer_cv.split(X_torch):\n",
    "    X_train_val_outer, X_test = X_torch[train_val_idx], X_torch[test_idx]\n",
    "    y_train_val_outer, y_test = y_torch[train_val_idx], y_torch[test_idx]\n",
    "\n",
    "    best_lambda = None\n",
    "    best_val_error = float('inf')\n",
    "    best_model = None\n",
    "\n",
    "    # Inner cross-validation loop\n",
    "    for train_idx, val_idx in inner_cv.split(X_train_val_outer):\n",
    "        X_train, X_val = X_train_val_outer[train_idx], X_train_val_outer[val_idx]\n",
    "        y_train, y_val = y_train_val_outer[train_idx], y_train_val_outer[val_idx]\n",
    "\n",
    "        # Loop over lambda values\n",
    "        for lambda_ in lambdas:\n",
    "            model = LogisticRegressionModel(input_dim=X.shape[1])\n",
    "            criterion = nn.BCELoss()\n",
    "            optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=lambda_)\n",
    "\n",
    "            # Train the model\n",
    "            model.train()\n",
    "            for epoch in range(100):\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X_train)\n",
    "                loss = criterion(outputs, y_train)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Evaluate the model on the validation set\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                predictions = model(X_val)\n",
    "                predictions = (predictions > 0.5).float()\n",
    "                val_error = 1 - torch.mean((predictions == y_val).float()).item()\n",
    "\n",
    "            if val_error < best_val_error:\n",
    "                best_val_error = val_error\n",
    "                best_model = model\n",
    "                best_lambda = lambda_\n",
    "\n",
    "    # Train the best model with the best lambda on the entire training set of the outer fold\n",
    "    best_model.train()\n",
    "    for epoch in range(100):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = best_model(X_train_val_outer)\n",
    "        loss = criterion(outputs, y_train_val_outer)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    best_model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = best_model(X_test)\n",
    "        predictions = (predictions > 0.5).float()\n",
    "        test_error = 1 - torch.mean((predictions == y_test).float()).item()\n",
    "    test_errors.append(test_error)\n",
    "\n",
    "    print('best lambda', best_lambda, 'test error', test_error)\n",
    "\n",
    "# Compute the mean test error across all outer folds\n",
    "mean_test_error = np.mean(test_errors)\n",
    "print('mean test error', mean_test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best hidden units 9 test error 0.42222219705581665\n",
      "best hidden units 3 test error 0.42222219705581665\n",
      "best hidden units 14 test error 0.3863636255264282\n",
      "best hidden units 10 test error 0.27272725105285645\n",
      "best hidden units 8 test error 0.3181818127632141\n",
      "best hidden units 6 test error 0.22727274894714355\n",
      "best hidden units 6 test error 0.3863636255264282\n",
      "best hidden units 7 test error 0.27272725105285645\n",
      "best hidden units 8 test error 0.3863636255264282\n",
      "best hidden units 5 test error 0.47727274894714355\n",
      "mean test error 0.35717170834541323\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import KFold\n",
    "from data import *\n",
    "np.random.seed(2)\n",
    "torch.manual_seed(2)\n",
    "\n",
    "class ANNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_units):\n",
    "        super(ANNModel, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, hidden_units)\n",
    "        self.layer2 = nn.Linear(hidden_units, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "hidden_units_values = [x for x in range(1, 15)]  # Define a range of hidden unit values\n",
    "\n",
    "# Convert the numpy arrays to PyTorch tensors\n",
    "X_torch = torch.from_numpy(X.astype(np.float32))\n",
    "\n",
    "# Convert the target to binary labels\n",
    "y = (y > np.median(y)).astype(int)  # 1 if y is above the median, 0 otherwise\n",
    "y_torch = torch.from_numpy(y.astype(np.float32)).view(-1, 1)\n",
    "\n",
    "K1, K2 = 10, 10  # Number of folds in the outer and inner cross-validation loops respectively\n",
    "outer_cv = KFold(n_splits=K1, shuffle=True)\n",
    "inner_cv = KFold(n_splits=K2, shuffle=True)\n",
    "\n",
    "test_errors = []\n",
    "\n",
    "# Outer cross-validation loop\n",
    "for train_val_idx, test_idx in outer_cv.split(X_torch):\n",
    "    X_train_val_outer, X_test = X_torch[train_val_idx], X_torch[test_idx]\n",
    "    y_train_val_outer, y_test = y_torch[train_val_idx], y_torch[test_idx]\n",
    "\n",
    "    best_hidden_units = None\n",
    "    best_val_error = float('inf')\n",
    "    best_model = None\n",
    "\n",
    "    # Inner cross-validation loop\n",
    "    for train_idx, val_idx in inner_cv.split(X_train_val_outer):\n",
    "        X_train, X_val = X_train_val_outer[train_idx], X_train_val_outer[val_idx]\n",
    "        y_train, y_val = y_train_val_outer[train_idx], y_train_val_outer[val_idx]\n",
    "\n",
    "        # Loop over hidden unit values\n",
    "        for hidden_units in hidden_units_values:\n",
    "            model = ANNModel(input_dim=X.shape[1], hidden_units=hidden_units)\n",
    "            criterion = nn.BCELoss()\n",
    "            optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "            # Train the model\n",
    "            model.train()\n",
    "            for epoch in range(100):\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X_train)\n",
    "                loss = criterion(outputs, y_train)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Evaluate the model on the validation set\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                predictions = model(X_val)\n",
    "                predictions = (predictions > 0.5).float()\n",
    "                val_error = 1 - torch.mean((predictions == y_val).float()).item()\n",
    "\n",
    "\n",
    "            if val_error < best_val_error:\n",
    "                best_val_error = val_error\n",
    "                best_model = model\n",
    "                best_hidden_units = hidden_units\n",
    "\n",
    "    # Train the best model with the best hidden units on the entire training set of the outer fold\n",
    "    best_model.train()\n",
    "    for epoch in range(100):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = best_model(X_train_val_outer)\n",
    "        loss = criterion(outputs, y_train_val_outer)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    best_model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = best_model(X_test)\n",
    "        predictions = (predictions > 0.5).float()\n",
    "        test_error = 1 - torch.mean((predictions == y_test).float()).item()\n",
    "    test_errors.append(test_error)\n",
    "\n",
    "    print('best hidden units', best_hidden_units, 'test error', test_error)\n",
    "\n",
    "# Compute the mean test error across all outer folds\n",
    "mean_test_error = np.mean(test_errors)\n",
    "print('mean test error', mean_test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error 0.5333333333333333\n",
      "test error 0.5555555555555556\n",
      "test error 0.5681818181818181\n",
      "test error 0.5\n",
      "test error 0.6363636363636364\n",
      "test error 0.5227272727272727\n",
      "test error 0.6136363636363636\n",
      "test error 0.5227272727272727\n",
      "test error 0.5454545454545454\n",
      "test error 0.5681818181818181\n",
      "mean baseline error 0.5566161616161616\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from data import *\n",
    "\n",
    "# Convert the target back to numpy array for sklearn\n",
    "y_np = y_torch.numpy()\n",
    "\n",
    "# Create the dummy classifier\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "\n",
    "# Outer cross-validation loop\n",
    "baseline_errors = []\n",
    "for train_val_idx, test_idx in outer_cv.split(X_torch):\n",
    "    X_train_val_outer, X_test = X_torch[train_val_idx], X_torch[test_idx]\n",
    "    y_train_val_outer, y_test = y_np[train_val_idx], y_np[test_idx]\n",
    "\n",
    "    # Train the dummy classifier\n",
    "    dummy_clf.fit(X_train_val_outer, y_train_val_outer)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = dummy_clf.predict(X_test)\n",
    "\n",
    "    # Compute the error\n",
    "    test_error = 1 - accuracy_score(y_test, y_pred)\n",
    "    baseline_errors.append(test_error)\n",
    "\n",
    "    print('test error', test_error)\n",
    "\n",
    "# Compute the mean test error across all outer folds\n",
    "mean_baseline_error = np.mean(baseline_errors)\n",
    "print('mean baseline error', mean_baseline_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
