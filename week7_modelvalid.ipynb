{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Location of the iris.csv file: c:\\Users\\vietn\\anaconda3\\envs\\ml\\Lib\\site-packages\\dtuimldmtools\\data\\iris.csv\n",
      "Crossvalidation fold: 1/150\n",
      "Crossvalidation fold: 2/150\n",
      "Crossvalidation fold: 3/150\n",
      "Crossvalidation fold: 4/150\n",
      "Crossvalidation fold: 5/150\n",
      "Crossvalidation fold: 6/150\n",
      "Crossvalidation fold: 7/150\n",
      "Crossvalidation fold: 8/150\n",
      "Crossvalidation fold: 9/150\n",
      "Crossvalidation fold: 10/150\n",
      "Crossvalidation fold: 11/150\n",
      "Crossvalidation fold: 12/150\n",
      "Crossvalidation fold: 13/150\n",
      "Crossvalidation fold: 14/150\n",
      "Crossvalidation fold: 15/150\n",
      "Crossvalidation fold: 16/150\n",
      "Crossvalidation fold: 17/150\n",
      "Crossvalidation fold: 18/150\n",
      "Crossvalidation fold: 19/150\n",
      "Crossvalidation fold: 20/150\n",
      "Crossvalidation fold: 21/150\n",
      "Crossvalidation fold: 22/150\n",
      "Crossvalidation fold: 23/150\n",
      "Crossvalidation fold: 24/150\n",
      "Crossvalidation fold: 25/150\n",
      "Crossvalidation fold: 26/150\n",
      "Crossvalidation fold: 27/150\n",
      "Crossvalidation fold: 28/150\n",
      "Crossvalidation fold: 29/150\n",
      "Crossvalidation fold: 30/150\n",
      "Crossvalidation fold: 31/150\n",
      "Crossvalidation fold: 32/150\n",
      "Crossvalidation fold: 33/150\n",
      "Crossvalidation fold: 34/150\n",
      "Crossvalidation fold: 35/150\n",
      "Crossvalidation fold: 36/150\n",
      "Crossvalidation fold: 37/150\n",
      "Crossvalidation fold: 38/150\n",
      "Crossvalidation fold: 39/150\n",
      "Crossvalidation fold: 40/150\n",
      "Crossvalidation fold: 41/150\n",
      "Crossvalidation fold: 42/150\n",
      "Crossvalidation fold: 43/150\n",
      "Crossvalidation fold: 44/150\n",
      "Crossvalidation fold: 45/150\n",
      "Crossvalidation fold: 46/150\n",
      "Crossvalidation fold: 47/150\n",
      "Crossvalidation fold: 48/150\n",
      "Crossvalidation fold: 49/150\n",
      "Crossvalidation fold: 50/150\n",
      "Crossvalidation fold: 51/150\n",
      "Crossvalidation fold: 52/150\n",
      "Crossvalidation fold: 53/150\n",
      "Crossvalidation fold: 54/150\n",
      "Crossvalidation fold: 55/150\n",
      "Crossvalidation fold: 56/150\n",
      "Crossvalidation fold: 57/150\n",
      "Crossvalidation fold: 58/150\n",
      "Crossvalidation fold: 59/150\n",
      "Crossvalidation fold: 60/150\n",
      "Crossvalidation fold: 61/150\n",
      "Crossvalidation fold: 62/150\n",
      "Crossvalidation fold: 63/150\n",
      "Crossvalidation fold: 64/150\n",
      "Crossvalidation fold: 65/150\n",
      "Crossvalidation fold: 66/150\n",
      "Crossvalidation fold: 67/150\n",
      "Crossvalidation fold: 68/150\n",
      "Crossvalidation fold: 69/150\n",
      "Crossvalidation fold: 70/150\n",
      "Crossvalidation fold: 71/150\n",
      "Crossvalidation fold: 72/150\n",
      "Crossvalidation fold: 73/150\n",
      "Crossvalidation fold: 74/150\n",
      "Crossvalidation fold: 75/150\n",
      "Crossvalidation fold: 76/150\n",
      "Crossvalidation fold: 77/150\n",
      "Crossvalidation fold: 78/150\n",
      "Crossvalidation fold: 79/150\n",
      "Crossvalidation fold: 80/150\n",
      "Crossvalidation fold: 81/150\n",
      "Crossvalidation fold: 82/150\n",
      "Crossvalidation fold: 83/150\n",
      "Crossvalidation fold: 84/150\n",
      "Crossvalidation fold: 85/150\n",
      "Crossvalidation fold: 86/150\n",
      "Crossvalidation fold: 87/150\n",
      "Crossvalidation fold: 88/150\n",
      "Crossvalidation fold: 89/150\n",
      "Crossvalidation fold: 90/150\n",
      "Crossvalidation fold: 91/150\n",
      "Crossvalidation fold: 92/150\n",
      "Crossvalidation fold: 93/150\n",
      "Crossvalidation fold: 94/150\n",
      "Crossvalidation fold: 95/150\n",
      "Crossvalidation fold: 96/150\n",
      "Crossvalidation fold: 97/150\n",
      "Crossvalidation fold: 98/150\n",
      "Crossvalidation fold: 99/150\n",
      "Crossvalidation fold: 100/150\n",
      "Crossvalidation fold: 101/150\n",
      "Crossvalidation fold: 102/150\n",
      "Crossvalidation fold: 103/150\n",
      "Crossvalidation fold: 104/150\n",
      "Crossvalidation fold: 105/150\n",
      "Crossvalidation fold: 106/150\n",
      "Crossvalidation fold: 107/150\n",
      "Crossvalidation fold: 108/150\n",
      "Crossvalidation fold: 109/150\n",
      "Crossvalidation fold: 110/150\n",
      "Crossvalidation fold: 111/150\n",
      "Crossvalidation fold: 112/150\n",
      "Crossvalidation fold: 113/150\n",
      "Crossvalidation fold: 114/150\n",
      "Crossvalidation fold: 115/150\n",
      "Crossvalidation fold: 116/150\n",
      "Crossvalidation fold: 117/150\n",
      "Crossvalidation fold: 118/150\n",
      "Crossvalidation fold: 119/150\n",
      "Crossvalidation fold: 120/150\n",
      "Crossvalidation fold: 121/150\n",
      "Crossvalidation fold: 122/150\n",
      "Crossvalidation fold: 123/150\n",
      "Crossvalidation fold: 124/150\n",
      "Crossvalidation fold: 125/150\n",
      "Crossvalidation fold: 126/150\n",
      "Crossvalidation fold: 127/150\n",
      "Crossvalidation fold: 128/150\n",
      "Crossvalidation fold: 129/150\n",
      "Crossvalidation fold: 130/150\n",
      "Crossvalidation fold: 131/150\n",
      "Crossvalidation fold: 132/150\n",
      "Crossvalidation fold: 133/150\n",
      "Crossvalidation fold: 134/150\n",
      "Crossvalidation fold: 135/150\n",
      "Crossvalidation fold: 136/150\n",
      "Crossvalidation fold: 137/150\n",
      "Crossvalidation fold: 138/150\n",
      "Crossvalidation fold: 139/150\n",
      "Crossvalidation fold: 140/150\n",
      "Crossvalidation fold: 141/150\n",
      "Crossvalidation fold: 142/150\n",
      "Crossvalidation fold: 143/150\n",
      "Crossvalidation fold: 144/150\n",
      "Crossvalidation fold: 145/150\n",
      "Crossvalidation fold: 146/150\n",
      "Crossvalidation fold: 147/150\n",
      "Crossvalidation fold: 148/150\n",
      "Crossvalidation fold: 149/150\n",
      "Crossvalidation fold: 150/150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# requires data from exercise 1.5.1\n",
    "from ex1_5_1 import *\n",
    "from matplotlib.pyplot import figure, plot, show, xlabel, ylabel\n",
    "from scipy.io import loadmat\n",
    "from sklearn import model_selection\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# This script crates predictions from three KNN classifiers using cross-validation\n",
    "\n",
    "# Maximum number of neighbors\n",
    "L = [1, 20, 80]\n",
    "\n",
    "CV = model_selection.LeaveOneOut()\n",
    "i = 0\n",
    "\n",
    "# store predictions.\n",
    "yhat = []\n",
    "y_true = []\n",
    "for train_index, test_index in CV.split(X, y):\n",
    "    print(\"Crossvalidation fold: {0}/{1}\".format(i + 1, N))\n",
    "\n",
    "    # extract training and test set for current CV fold\n",
    "    X_train = X[train_index, :]\n",
    "    y_train = y[train_index]\n",
    "    X_test = X[test_index, :]\n",
    "    y_test = y[test_index]\n",
    "\n",
    "    # Fit classifier and classify the test points (consider 1 to 40 neighbors)\n",
    "    dy = []\n",
    "    for l in L:\n",
    "        knclassifier = KNeighborsClassifier(n_neighbors=l)\n",
    "        knclassifier.fit(X_train, y_train)\n",
    "        y_est = knclassifier.predict(X_test)\n",
    "\n",
    "        dy.append(y_est)\n",
    "        # errors[i,l-1] = np.sum(y_est[0]!=y_test[0])\n",
    "    dy = np.stack(dy, axis=1)\n",
    "    yhat.append(dy)\n",
    "    y_true.append(y_test)\n",
    "    i += 1\n",
    "\n",
    "yhat = np.concatenate(yhat)\n",
    "y_true = np.concatenate(y_true)\n",
    "yhat[:, 0]  # predictions made by first classifier.\n",
    "# Compute accuracy here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta point estimate 0.956953642384106  CI:  (0.9194225123023887, 0.9831344032786383)\n"
     ]
    }
   ],
   "source": [
    "from ex7_1_1 import *\n",
    "\n",
    "from dtuimldmtools import jeffrey_interval\n",
    "\n",
    "# Compute the Jeffreys interval\n",
    "alpha = 0.05\n",
    "[thetahatA, CIA] = jeffrey_interval(y_true, yhat[:, 0], alpha=alpha)\n",
    "\n",
    "print(\"Theta point estimate\", thetahatA, \" CI: \", CIA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ex7_1_1 import *\n",
    "\n",
    "from dtuimldmtools import mcnemar\n",
    "\n",
    "# Compute the Jeffreys interval\n",
    "alpha = 0.05\n",
    "[thetahat, CI, p] = mcnemar(y_true, yhat[:, 0], yhat[:, 1], alpha=alpha)\n",
    "\n",
    "print(\"theta = theta_A-theta_B point estimate\", thetahat, \" CI: \", CI, \"p-value\", p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00852969])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "import scipy.stats as st\n",
    "import sklearn.tree\n",
    "\n",
    "# requires data from exercise 1.5.1\n",
    "from ex5_1_5 import *\n",
    "from matplotlib.pyplot import figure, plot, show, xlabel, ylabel\n",
    "from sklearn import model_selection\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "X, y = X[:, :10], X[:, 10:]\n",
    "# This script crates predictions from three KNN classifiers using cross-validation\n",
    "\n",
    "test_proportion = 0.2\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    X, y, test_size=test_proportion\n",
    ")\n",
    "\n",
    "mA = sklearn.linear_model.LinearRegression().fit(X_train, y_train)\n",
    "mB = sklearn.tree.DecisionTreeRegressor().fit(X_train, y_train)\n",
    "\n",
    "yhatA = mA.predict(X_test)\n",
    "yhatB = mB.predict(X_test)[:, np.newaxis]  #  justsklearnthings\n",
    "\n",
    "# perform statistical comparison of the models\n",
    "# compute z with squared error.\n",
    "zA = np.abs(y_test - yhatA) ** 2\n",
    "\n",
    "# compute confidence interval of model A\n",
    "alpha = 0.05\n",
    "CIA = st.t.interval(\n",
    "    1 - alpha, df=len(zA) - 1, loc=np.mean(zA), scale=st.sem(zA)\n",
    ")  # Confidence interval\n",
    "\n",
    "# Compute confidence interval of z = zA-zB and p-value of Null hypothesis\n",
    "zB = np.abs(y_test - yhatB) ** 2\n",
    "z = zA - zB\n",
    "CI = st.t.interval(\n",
    "    1 - alpha, len(z) - 1, loc=np.mean(z), scale=st.sem(z)\n",
    ")  # Confidence interval\n",
    "p = 2 * st.t.cdf(-np.abs(np.mean(z)) / st.sem(z), df=len(z) - 1)  # p-value\n",
    "\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0002860926897008202, 5.004483513549672e-65]\n",
      "(-0.28765271788606883, -0.12465880710403474) (-0.22970625442913745, -0.18266688025409783)\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as st\n",
    "import sklearn.linear_model\n",
    "import sklearn.tree\n",
    "\n",
    "# requires data from exercise 1.5.1\n",
    "from ex5_1_5 import *\n",
    "from sklearn import model_selection\n",
    "\n",
    "from dtuimldmtools import *\n",
    "from dtuimldmtools.statistics.statistics import correlated_ttest\n",
    "\n",
    "loss = 2\n",
    "X,y = X[:,:10], X[:,10:]\n",
    "# This script crates predictions from three KNN classifiers using cross-validation\n",
    "\n",
    "K = 10 # We presently set J=K\n",
    "m = 1\n",
    "r = []\n",
    "kf = model_selection.KFold(n_splits=K)\n",
    "\n",
    "for dm in range(m):\n",
    "    y_true = []\n",
    "    yhat = []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, y_train = X[train_index,:], y[train_index]\n",
    "        X_test, y_test = X[test_index, :], y[test_index]\n",
    "\n",
    "        mA = sklearn.linear_model.LinearRegression().fit(X_train, y_train)\n",
    "        mB = sklearn.tree.DecisionTreeRegressor().fit(X_train, y_train)\n",
    "\n",
    "        yhatA = mA.predict(X_test)\n",
    "        yhatB = mB.predict(X_test)[:, np.newaxis]  # justsklearnthings\n",
    "        y_true.append(y_test)\n",
    "        yhat.append( np.concatenate([yhatA, yhatB], axis=1) )\n",
    "\n",
    "        r.append( np.mean( np.abs( yhatA-y_test ) ** loss - np.abs( yhatB-y_test) ** loss ) )\n",
    "\n",
    "# Initialize parameters and run test appropriate for setup II\n",
    "alpha = 0.05\n",
    "rho = 1/K\n",
    "p_setupII, CI_setupII = correlated_ttest(r, rho, alpha=alpha)\n",
    "\n",
    "if m == 1:\n",
    "    y_true = np.concatenate(y_true)[:,0]\n",
    "    yhat = np.concatenate(yhat)\n",
    "\n",
    "    # note our usual setup I ttest only makes sense if m=1.\n",
    "    zA = np.abs(y_true - yhat[:,0] ) ** loss\n",
    "    zB = np.abs(y_true - yhat[:,1] ) ** loss\n",
    "    z = zA - zB\n",
    "\n",
    "    CI_setupI = st.t.interval(1 - alpha, len(z) - 1, loc=np.mean(z), scale=st.sem(z))  # Confidence interval\n",
    "    p_setupI = st.t.cdf(-np.abs(np.mean(z)) / st.sem(z), df=len(z) - 1)  # p-value\n",
    "\n",
    "    print( [p_setupII, p_setupI] )\n",
    "    print(CI_setupII, CI_setupI )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran Exercise 7.2.3\n"
     ]
    }
   ],
   "source": [
    "# exercise 7.4.3\n",
    "import importlib_resources\n",
    "import numpy as np\n",
    "\n",
    "# Load list of names from files\n",
    "\n",
    "fmale = open(importlib_resources.files(\"dtuimldmtools\").joinpath(\"data/male.txt\"), \"r\")\n",
    "ffemale = open(\n",
    "    importlib_resources.files(\"dtuimldmtools\").joinpath(\"data/female.txt\"), \"r\"\n",
    ")\n",
    "mnames = fmale.readlines()\n",
    "fnames = ffemale.readlines()\n",
    "names = mnames + fnames\n",
    "gender = [0] * len(mnames) + [1] * len(fnames)\n",
    "fmale.close()\n",
    "ffemale.close()\n",
    "\n",
    "# Extract X, y and the rest of variables. Include only names of >4 characters.\n",
    "X = np.zeros((len(names), 4))\n",
    "y = np.zeros((len(names), 1))\n",
    "n = 0\n",
    "for i in range(0, len(names)):\n",
    "    name = names[i].strip().lower()\n",
    "    if len(name) > 3:\n",
    "        X[n, :] = [\n",
    "            ord(name[0]) - ord(\"a\") + 1,\n",
    "            ord(name[1]) - ord(\"a\") + 1,\n",
    "            ord(name[-2]) - ord(\"a\") + 1,\n",
    "            ord(name[-1]) - ord(\"a\") + 1,\n",
    "        ]\n",
    "        y[n, 0] = gender[i]\n",
    "        n += 1\n",
    "X = X[0:n, :]\n",
    "y = y[0:n, :]\n",
    "\n",
    "N, M = X.shape\n",
    "C = 2\n",
    "attributeNames = [\"1st letter\", \"2nd letter\", \"Next-to-last letter\", \"Last letter\"]\n",
    "classNames = [\"Female\", \"Male\"]\n",
    "\n",
    "print(\"Ran Exercise 7.2.3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate: 20.990614174303314%\n",
      "Ran Exercise 7.2.4\n"
     ]
    }
   ],
   "source": [
    "# exercise 7.4.4w\n",
    "import numpy as np\n",
    "from ex7_4_3 import *\n",
    "from sklearn import model_selection\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "np.random.seed(2450)\n",
    "y = y.squeeze()\n",
    "0\n",
    "# Naive Bayes classifier parameters\n",
    "alpha = 1.0  # pseudo-count, additive parameter (Laplace correction if 1.0 or Lidtstone smoothing otherwise)\n",
    "fit_prior = True  # uniform prior (change to True to estimate prior from data)\n",
    "\n",
    "# K-fold crossvalidation\n",
    "K = 10\n",
    "CV = model_selection.KFold(n_splits=K, shuffle=True)\n",
    "\n",
    "X = X[:, 0:4]  # using all 4 letters,\n",
    "# for using e.g. only third letter or first and last try X[:,[2]] and X[:, [0,3]]\n",
    "\n",
    "# We need to specify that the data is categorical.\n",
    "# MultinomialNB does not have this functionality, but we can achieve similar\n",
    "# results by doing a one-hot-encoding - the intermediate steps in in training\n",
    "# the classifier are off, but the final result is corrent.\n",
    "# If we didn't do the converstion MultinomialNB assumes that the numbers are\n",
    "# e.g. discrete counts of tokens. Without the encoding, the value 26 wouldn't\n",
    "# mean \"the token 'z'\", but it would mean 26 counts of some token,\n",
    "# resulting in 1 and 2 meaning a difference in one count of a given token as\n",
    "# opposed to the desired 'a' versus 'b'.\n",
    "X = OneHotEncoder().fit_transform(X=X)\n",
    "\n",
    "errors = np.zeros(K)\n",
    "k = 0\n",
    "for train_index, test_index in CV.split(X):\n",
    "    # print('Crossvalidation fold: {0}/{1}'.format(k+1,K))\n",
    "\n",
    "    # extract training and test set for current CV fold\n",
    "    X_train = X[train_index, :]\n",
    "    y_train = y[train_index]\n",
    "    X_test = X[test_index, :]\n",
    "    y_test = y[test_index]\n",
    "\n",
    "    nb_classifier = MultinomialNB(alpha=alpha, fit_prior=fit_prior)\n",
    "    nb_classifier.fit(X_train, y_train)\n",
    "    y_est_prob = nb_classifier.predict_proba(X_test)\n",
    "    y_est = np.argmax(y_est_prob, 1)\n",
    "\n",
    "    errors[k] = np.sum(y_est != y_test, dtype=float) / y_test.shape[0]\n",
    "    k += 1\n",
    "\n",
    "# Plot the classification error rate\n",
    "print(\"Error rate: {0}%\".format(100 * np.mean(errors)))\n",
    "\n",
    "print(\"Ran Exercise 7.2.4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
